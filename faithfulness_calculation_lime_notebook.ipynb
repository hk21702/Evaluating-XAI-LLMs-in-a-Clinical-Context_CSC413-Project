{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Comprehensiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Faithfulness on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/u11/c0/00/ammcourt/miniconda3/envs/csc413/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    OPTForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import wandb\n",
    "\n",
    "MODEL = \"facebook/opt-350m\"\n",
    "MAX_POSITION_EMBEDDINGS = 2048\n",
    "\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"OPT-350m-mimic-full\"\n",
    "VAL_DATASET_PATH = \"data/val_9.csv\"\n",
    "CODE_PATH = \"data/icd9_codes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True, device=device)\n",
    "\n",
    "code_labels = pd.read_csv(\"data/icd9_codes.csv\")\n",
    "dataset = load_dataset(\"csv\", data_files=VAL_DATASET_PATH)\n",
    "\n",
    "# Create class dictionaries\n",
    "classes = [class_ for class_ in code_labels[\"icd_code\"] if class_]\n",
    "class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "id2class = {id: class_ for class_, id in class2id.items()}\n",
    "\n",
    "\n",
    "def multi_labels_to_ids(labels: list[str]) -> list[float]:\n",
    "    ids = [0.0] * len(class2id)  # BCELoss requires float as target type\n",
    "    for label in labels:\n",
    "        ids[class2id[label]] = 1.0\n",
    "    return ids\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    result = tokenizer(\n",
    "        example[\"text\"], truncation=True, max_length=MAX_POSITION_EMBEDDINGS\n",
    "    )\n",
    "    result[\"labels\"] = [multi_labels_to_ids(eval(label)) for label in example[\"labels\"]]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    return_unused_kwargs=True,\n",
    ")\n",
    "\n",
    "if unused_kwargs:\n",
    "    print(f\"Unused kwargs: {unused_kwargs}\")\n",
    "\n",
    "model = OPTForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    config=config,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_dataset = load_dataset(\"csv\", data_files=VAL_DATASET_PATH)\n",
    "\n",
    "print(untokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime.lime_text import IndexedString\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=classes, bow=False)\n",
    "\n",
    "def predictor_opt(texts):\n",
    "    print(len(texts))\n",
    "    tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    outputs = model(**tk)\n",
    "    tensor_logits = outputs[0]\n",
    "    probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    del tk, tensor_logits\n",
    "    # probas = F.sigmoid(tensor_logits).detach().cpu().numpy()\n",
    "    return probas\n",
    "\n",
    "\n",
    "def predictor_model(texts, model, tokenizer):\n",
    "    print(len(texts))\n",
    "    tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    outputs = model(**tk)\n",
    "    tensor_logits = outputs[0]\n",
    "    probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    del tk, tensor_logits\n",
    "    # probas = F.sigmoid(tensor_logits).detach().cpu().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instances are formatted as a list of strings, where each string is one word used by lime. The rationales mask is a list of indices, where the first list refers to the index of the sample the label corresponds to and the second list is the index of string used in that rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for faithfulness calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test with 10 instances for faithfulness evaluation\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# get the lime evaluations of each instance\n",
    "import faithfulness\n",
    "# this reimports the library for easy testing in the notebook\n",
    "import importlib\n",
    "importlib.reload(faithfulness)\n",
    "\n",
    "samples_start = 0\n",
    "samples_end = 50\n",
    "\n",
    "instances = untokenized_dataset[\"train\"][samples_start:samples_end][\"text\"]\n",
    "print(len(instances))\n",
    "\n",
    "# print(instances)\n",
    "explainer = LimeTextExplainer(class_names=classes, bow=False)\n",
    "\n",
    "indexed_text, index_array_rationalle = faithfulness.lime_create_index_arrays(instances, predictor_opt, explainer)\n",
    "# print(indexed_text)\n",
    "# print(index_array_rationalle)\n",
    "\n",
    "# # remove the rationale words\n",
    "rationalle_removed = faithfulness.remove_rationale_words(indexed_text, index_array_rationalle)\n",
    "others_removed = faithfulness.remove_other_words(indexed_text, index_array_rationalle)\n",
    "\n",
    "# rationalle_removed = rationalle_removed + rationalle_removed + rationalle_removed + rationalle_removed + rationalle_removed\n",
    "# others_removed = others_removed + others_removed + others_removed + others_removed + others_removed \n",
    "# instances = instances + instances + instances + instances + instances\n",
    "\n",
    "# print(rationalle_removed)\n",
    "\n",
    "# print(len(rationalle_removed))\n",
    "# print(len(others_removed))\n",
    "\n",
    "# the extra list is needed since the function expects a list of instances each coming from a different interpretability method\n",
    "# testing multi input by duplicating the arrays, don't actually do this\n",
    "ind, faith = faithfulness.calculate_faithfulness(instances, [rationalle_removed, rationalle_removed], [others_removed, others_removed], model, tokenizer, predictor_model)\n",
    "print(ind)\n",
    "print(faith)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
