{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/u11/c0/00/ammcourt/miniconda3/envs/csc413/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    OPTForSequenceClassification,\n",
    "    Pipeline,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "\n",
    "MODEL = \"facebook/opt-350m\"\n",
    "MAX_POSITION_EMBEDDINGS = 2048\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"OPT-350m-mimic-full\"\n",
    "VAL_DATASET_PATH = \"data/val_9.csv\"\n",
    "CODE_PATH = \"data/icd9_codes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True, device=device)\n",
    "\n",
    "code_labels = pd.read_csv(\"data/icd9_codes.csv\")\n",
    "dataset = load_dataset(\"csv\", data_files=VAL_DATASET_PATH)\n",
    "\n",
    "# Create class dictionaries\n",
    "classes = [class_ for class_ in code_labels[\"icd_code\"] if class_]\n",
    "class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "id2class = {id: class_ for class_, id in class2id.items()}\n",
    "\n",
    "\n",
    "def multi_labels_to_ids(labels: list[str]) -> list[float]:\n",
    "    ids = [0.0] * len(class2id)  # BCELoss requires float as target type\n",
    "    for label in labels:\n",
    "        ids[class2id[label]] = 1.0\n",
    "    return ids\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    result = tokenizer(\n",
    "        example[\"text\"], truncation=True, max_length=MAX_POSITION_EMBEDDINGS\n",
    "    )\n",
    "    result[\"labels\"] = [multi_labels_to_ids(eval(label)) for label in example[\"labels\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess_function, load_from_cache_file=True, batched=True, num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OPTForSequenceClassification(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttentionLayerBetterTransformer(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=512, out_features=51, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    return_unused_kwargs=True,\n",
    ")\n",
    "\n",
    "if unused_kwargs:\n",
    "    print(f\"Unused kwargs: {unused_kwargs}\")\n",
    "\n",
    "model = OPTForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    config=config,\n",
    ").to(device)\n",
    "\n",
    "model.load_adapter(CHECKPOINT_DIR)\n",
    "model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_dataset = load_dataset(\"csv\", data_files=VAL_DATASET_PATH)\n",
    "\n",
    "print(untokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    untokenized_dataset[\"train\"][0][\"text\"],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_POSITION_EMBEDDINGS,\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT_ICD9_Pipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_POSITION_EMBEDDINGS,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        # logits = model_outputs.logits[0].numpy()\n",
    "        #print(logits)\n",
    "        probs = model_outputs[\"logits\"].sigmoid()\n",
    "\n",
    "        output = []\n",
    "        for i, prob in enumerate(probs[0]):\n",
    "            label = self.model.config.id2label[i]\n",
    "            score = prob\n",
    "            output.append({\"label\": label, \"score\": score})\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = OPT_ICD9_Pipeline(model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(untokenized_dataset[\"train\"][2][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(pipeline.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = shap.sample(untokenized_dataset[\"train\"][\"text\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(pipeline, masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_dataset[\"train\"][:2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(untokenized_dataset[\"train\"][:5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[0, :, \"d-2749\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = shap.plots.text(shap_values[0, :, \"d-2749\"], display = False)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shap_values)\n",
    "\n",
    "print(explainer.feature_names)\n",
    "print(len(shap_values.values[1]))\n",
    "# print(shap_values[0, :, \"d-2749\"].values)\n",
    "print(len(shap_values[0, :, \"d-2749\"].values))\n",
    "# print(len(shap_values.values[0]))\n",
    "# print(len(shap_values.base_values[0]))\n",
    "# print(len(shap_values.data[0]))\n",
    "\n",
    "# print(shap_values.values[0])\n",
    "# print(shap_values.base_values[0])\n",
    "# print(shap_values.data[0])\n",
    "\n",
    "max_indices = []\n",
    "for val in shap_values.values[0]:\n",
    "    print(val)\n",
    "    inde = np.argmax(val)\n",
    "    if inde not in max_indices:\n",
    "        max_indices.append(inde)\n",
    "    \n",
    "print(max_indices)\n",
    "\n",
    "# feature_names = untokenized_dataset[\"train\"].columns\n",
    "# rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "# vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "# shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "#                                   columns=['col_name','feature_importance_vals'])\n",
    "# shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "#                                ascending=False, inplace=True)\n",
    "# shap_importance.head()\n",
    "\n",
    "\n",
    "# values, clustering = unpack_shap_explanation_contents(v)\n",
    "#             tokens, values, group_sizes = process_shap_values(v.data, values, grouping_threshold, separator, clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap pipeline works in the following order:\n",
    "- Generate the indices array (note this array corresponds to tokens, not strings)\n",
    "- Tokenize the input dataset\n",
    "- Pass the tokenized input dataset to the masking functions along with the indices\n",
    "- The pass all tokenized and masked tokenized datasets to the faithfulness calcuation\n",
    "\n",
    "This requires a prediction function that expects a tokenized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_arrays_shap(inputs, pred_func, model, tokenizer, top_k = 5):\n",
    "    \"\"\" Function to create the arrays corresponding to the shap \n",
    "    \n",
    "    The output is in the format [[input_index_0, input_index_0, ... input_index_n, input_index_n], \n",
    "    [rationale_token_index_0 (for input 0), rationale_token_index_1 (for input 0), ... rationale_token_index_k-1 (for input n), rationale_token_index_k (for input n)]]. \n",
    "    This is used as an indexing array for masking.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the shap values over the inputs\n",
    "    # shap_values = explainer(inputs, batch_size=5)\n",
    "    \n",
    "    # get the mode inferences for the inputs\n",
    "    inferences = pred_func(inputs, model, tokenizer)\n",
    "    indices_array = None\n",
    "    # get the longest \n",
    "    \n",
    "    for i, val in enumerate(shap_values):\n",
    "        # get the choosen labels\n",
    "        print(\"Inferences: \", inferences)\n",
    "        choosen_labels = np.where(inferences[i] > 0.5)\n",
    "        choosen_labels = np.unique(choosen_labels)\n",
    "        \n",
    "        # convert the indices to labels\n",
    "        choosen_labels = np.array(choosen_labels).astype(int)\n",
    "        print(\"Choosen label: \", choosen_labels)\n",
    "        choosen_labels = [id2class[label] for label in choosen_labels]\n",
    "        print(id2class)\n",
    "        print(choosen_labels)\n",
    "        \n",
    "        # for each shap value, index in via it's choosen labels\n",
    "        total_top_k_indices = np.array([])\n",
    "        top_val = min(top_k, len(choosen_labels))\n",
    "        print(top_val)\n",
    "        print(choosen_labels)\n",
    "        \n",
    "        \n",
    "        for label in choosen_labels:\n",
    "            # get the top k shap value indices\n",
    "            print(label)\n",
    "            top_k_indices = np.argsort(shap_values[i, :, label].values)[-top_val:]\n",
    "            print(top_k_indices)\n",
    "            total_top_k_indices = np.append(total_top_k_indices, top_k_indices)\n",
    "        \n",
    "        # sort the indices array to be in ascending order\n",
    "        total_top_k_indices = np.sort(total_top_k_indices)\n",
    "        # remove duplicates\n",
    "        total_top_k_indices = np.unique(total_top_k_indices)\n",
    "        # this might be wrong, it seems like shap returns indices outside of the token range\n",
    "        # so I'm not sure if shap is using the same tokenization function as ours.\n",
    "        total_top_k_indices = total_top_k_indices[total_top_k_indices < 2048]\n",
    "        \n",
    "        # create a array of the same shape of total_top_k_indices and fill with value i\n",
    "        index_array = np.full(total_top_k_indices.shape, i)\n",
    "        \n",
    "        if i == 0:\n",
    "            indices_array = [index_array.tolist(), total_top_k_indices.tolist()]\n",
    "        else:\n",
    "            # append index array to indices array[0]\n",
    "            indices_array[0] = indices_array[0] + index_array.tolist()\n",
    "            # append total_top_k_indices to indices array[1]\n",
    "            indices_array[1] = indices_array[1] + total_top_k_indices.tolist()\n",
    "    \n",
    "    return np.array(indices_array).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_model_token(texts, model, tokenizer):\n",
    "    # print(len(texts))\n",
    "    tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    print(type(tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS)))\n",
    "    print(\"token_att: \", dir(tk))\n",
    "    outputs = model(**tk)\n",
    "    tensor_logits = outputs[0]\n",
    "    probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    return probas\n",
    "\n",
    "def predictor_model_no_token(texts, model, tokenizer):\n",
    "    # print(len(texts))\n",
    "    # tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    # tokenization is removed but still need to set texts to device\n",
    "    # i'm not sure why this is a list and don't have time to debug\n",
    "    print(\"Texts_type:\", type(texts))\n",
    "    print(\"Texts_dir:\",  dir(texts))\n",
    "    texts.to(device)\n",
    "    outputs = model(**texts)\n",
    "    tensor_logits = outputs[0]\n",
    "    probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faithfulness\n",
    "# this reimports the library for easy testing in the notebook\n",
    "import importlib\n",
    "importlib.reload(faithfulness)\n",
    "\n",
    "MAX_LEN=2048\n",
    "   \n",
    "# tokenize the test dataset\n",
    "test_data = untokenized_dataset[\"train\"][:5][\"text\"]\n",
    "\n",
    "inputs = tokenizer(test_data, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='pt')\n",
    "print(inputs['input_ids'][0][2:30])\n",
    "print(\"input type: \", type(inputs))\n",
    "\n",
    "indices_array = get_index_arrays_shap(test_data, predictor_model_token, model, tokenizer)\n",
    "print(indices_array)\n",
    "\n",
    "inputs_rationale_removed = faithfulness.remove_rationale_words(inputs, indices_array, join=False, tokenized=True)\n",
    "inputs_other_removed = faithfulness.remove_other_words(inputs, indices_array, join=False, tokenized=True)\n",
    "\n",
    "print(\"rational removed type: \", type(inputs_rationale_removed))\n",
    "print(\"other removed type: \", type(inputs_other_removed))\n",
    "\n",
    "ind, faith = faithfulness.calculate_faithfulness(inputs, [inputs_rationale_removed], [inputs_other_removed ], model, tokenizer, predictor_model_no_token, tokenized=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
