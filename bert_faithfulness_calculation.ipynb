{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Faithfulness on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import torch.nn.functional as F\n",
    "import shap\n",
    "from transformers import Pipeline\n",
    "import os \n",
    "import numpy\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "data_dir = \"output/\"\n",
    "destination_dir = \"./\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_short_path = \"data/test_10_top50_short.csv\"\n",
    "labels_10_top50 = pd.read_csv('data/icd10_codes_top50.csv')\n",
    "code_labels_10 = pd.read_csv(\"data/icd10_codes.csv\")\n",
    "print(\"dataset loaded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "CKPT = os.path.join(data_dir,\"best_model_state.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes\n",
      "bert model and tokenizer initialized\n"
     ]
    }
   ],
   "source": [
    "# Create class dictionaries\n",
    "classes = [class_ for class_ in code_labels_10[\"icd_code\"] if class_]\n",
    "class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "id2class = {id: class_ for class_, id in class2id.items()}\n",
    "\n",
    "print(\"classes\")\n",
    "\n",
    "config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    return_unused_kwargs=True,\n",
    ")\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL)\n",
    "model_bert = AutoModel.from_pretrained(MODEL, config=config, cache_dir='./model_ckpt/')\n",
    "print(\"bert model and tokenizer initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dcc48cedd549b1921d4a0fc889c36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer, length, classes):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = length\n",
    "        self.classes = classes\n",
    "        self.class2id = {class_: id for id, class_ in enumerate(self.classes)}\n",
    "        self.id2class = {id: class_ for class_, id in self.class2id.items()}\n",
    "        \n",
    "    def multi_labels_to_ids(self, labels: list[str]) -> list[float]:\n",
    "        ids = [0.0] * len(self.class2id)  # BCELoss requires float as target type\n",
    "        for label in labels:\n",
    "            ids[self.class2id[label]] = 1.0\n",
    "        return ids\n",
    "    \n",
    "    def tokenize_function(self, example):\n",
    "        result = self.tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        result[\"label\"] = torch.tensor([self.multi_labels_to_ids(eval(label)) for label in example[\"label\"]])\n",
    "        return result\n",
    "        \n",
    "data_files = {\n",
    "        \"test\": test_short_path,\n",
    "    }\n",
    "\n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer_bert, MAX_POSITION_EMBEDDINGS, classes)\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset = dataset.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=1)\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "print(\"dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.bert_model = model_bert\n",
    "        self.can_generate = model_bert.can_generate\n",
    "        self.base_model_prefix = model_bert.base_model_prefix\n",
    "        self.get_input_embeddings = model_bert.get_input_embeddings\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, 50)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "    \n",
    "model_bert = BERTClass()\n",
    "model_bert.load_state_dict(torch.load(CKPT))\n",
    "model_bert = model_bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime.lime_text import IndexedString\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=classes, bow=False)\n",
    "\n",
    "def predictor_opt(texts):\n",
    "    print(len(texts))\n",
    "    tk = tokenizer_bert(\n",
    "            texts,\n",
    "            max_length = MAX_POSITION_EMBEDDINGS,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "    ids = tk['input_ids'].to(device, dtype = torch.long)\n",
    "    mask = tk['attention_mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = tk['token_type_ids'].to(device, dtype = torch.long)\n",
    "    outputs = model_bert(ids, mask, token_type_ids)\n",
    "    # tensor_logits = outputs[0]\n",
    "    # probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    probas = F.sigmoid(outputs).detach().cpu().numpy()\n",
    "    del tk, outputs\n",
    "    # probas = F.sigmoid(tensor_logits).detach().cpu().numpy()\n",
    "    return probas\n",
    "\n",
    "\n",
    "def predictor_model(texts, model, tokenizer):\n",
    "    print(len(texts))\n",
    "    tk = tokenizer_bert(\n",
    "            texts,\n",
    "            max_length = MAX_POSITION_EMBEDDINGS,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "    ids = tk['input_ids'].to(device, dtype = torch.long)\n",
    "    mask = tk['attention_mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = tk['token_type_ids'].to(device, dtype = torch.long)\n",
    "    outputs = model_bert(ids, mask, token_type_ids)\n",
    "    # tensor_logits = outputs[0]\n",
    "    # probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    probas = F.sigmoid(outputs).detach().cpu().numpy()\n",
    "    del tk, outputs\n",
    "    # probas = F.sigmoid(tensor_logits).detach().cpu().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instances are formatted as a list of strings, where each string is one word used by lime. The rationales mask is a list of indices, where the first list refers to the index of the sample the label corresponds to and the second list is the index of string used in that rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for faithfulness calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# create a test with 10 instances for faithfulness evaluation\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# get the lime evaluations of each instance\n",
    "import faithfulness\n",
    "# this reimports the library for easy testing in the notebook\n",
    "import importlib\n",
    "importlib.reload(faithfulness)\n",
    "\n",
    "samples_start = 0\n",
    "samples_end = 50\n",
    "\n",
    "instances = dataset[\"test\"][samples_start:samples_end][\"text\"]\n",
    "# print(len(instances))\n",
    "\n",
    "# print(instances)\n",
    "explainer = LimeTextExplainer(class_names=classes, bow=False)\n",
    "\n",
    "indexed_text, index_array_rationalle = faithfulness.lime_create_index_arrays(instances, predictor_opt, explainer)\n",
    "# print(indexed_text)\n",
    "# print(index_array_rationalle)\n",
    "\n",
    "# # remove the rationale words\n",
    "rationalle_removed = faithfulness.remove_rationale_words(indexed_text, index_array_rationalle)\n",
    "others_removed = faithfulness.remove_other_words(indexed_text, index_array_rationalle)\n",
    "\n",
    "# rationalle_removed = rationalle_removed + rationalle_removed + rationalle_removed + rationalle_removed + rationalle_removed\n",
    "# others_removed = others_removed + others_removed + others_removed + others_removed + others_removed \n",
    "# instances = instances + instances + instances + instances + instances\n",
    "\n",
    "# print(rationalle_removed)\n",
    "\n",
    "# print(len(rationalle_removed))\n",
    "# print(len(others_removed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "Currently interpreting instance:  0\n",
      "Calculating Sufficiency\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "Calculating Comprehensiveness\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "\n",
      "-- Metrics -------------------------------------------------------------\n",
      "\n",
      "\n",
      "Faithfulness for iteration:  0.09531664\n",
      "Comprehensiveness for iteration:  0.41091752\n",
      "Sufficency for iteration:  0.23196052\n",
      "\n",
      "\n",
      "Comprehensiveness Median:  0.2714703\n",
      "Comprehensiveness q1 (25% percentile):  0.2267022393643856\n",
      "Comprehensiveness q3 (75% percentile):  0.4419192001223564\n",
      "\n",
      "\n",
      "Sufficency Median:  0.23101819\n",
      "Sufficency q1 (25% percentile):  0.19091162458062172\n",
      "Sufficency q3 (75% percentile):  0.25325431674718857\n",
      "\n",
      "Currently interpreting instance:  1\n",
      "Calculating Sufficiency\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "Calculating Comprehensiveness\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "\n",
      "-- Metrics -------------------------------------------------------------\n",
      "\n",
      "\n",
      "Faithfulness for iteration:  0.09601459\n",
      "Comprehensiveness for iteration:  0.42271277\n",
      "Sufficency for iteration:  0.22713907\n",
      "\n",
      "\n",
      "Comprehensiveness Median:  0.29238832\n",
      "Comprehensiveness q1 (25% percentile):  0.23402758315205574\n",
      "Comprehensiveness q3 (75% percentile):  0.46584709733724594\n",
      "\n",
      "\n",
      "Sufficency Median:  0.21825856\n",
      "Sufficency q1 (25% percentile):  0.1961136907339096\n",
      "Sufficency q3 (75% percentile):  0.25417785346508026\n",
      "\n",
      "0\n",
      "[0.09531664, 0.09601459]\n"
     ]
    }
   ],
   "source": [
    "# the extra list is needed since the function expects a list of instances each coming from a different interpretability method\n",
    "# testing multi input by duplicating the arrays, don't actually do this\n",
    "ind, faith = faithfulness.calculate_faithfulness(instances, [rationalle_removed, rationalle_removed], [others_removed, others_removed], model_bert, tokenizer_bert, predictor_model)\n",
    "print(ind)\n",
    "print(faith)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
