{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Faithfulness on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "import shap\n",
    "import shap\n",
    "from transformers import Pipeline\n",
    "\n",
    "import os \n",
    "import numpy\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "data_dir = \"output/\"\n",
    "destination_dir = \"./\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_short_path = \"data/test_10_top50_short.csv\"\n",
    "labels_10_top50 = pd.read_csv('data/icd10_codes_top50.csv')\n",
    "code_labels_10 = pd.read_csv(\"data/icd10_codes.csv\")\n",
    "print(\"dataset loaded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "MODEL = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "CKPT = os.path.join(data_dir,\"best_model_state.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes\n",
      "bert model and tokenizer initialized\n"
     ]
    }
   ],
   "source": [
    "# Create class dictionaries\n",
    "classes = [class_ for class_ in code_labels_10[\"icd_code\"] if class_]\n",
    "class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "id2class = {id: class_ for class_, id in class2id.items()}\n",
    "\n",
    "print(\"classes\")\n",
    "\n",
    "config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    return_unused_kwargs=True,\n",
    ")\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL)\n",
    "model_bert = AutoModel.from_pretrained(MODEL, config=config, cache_dir='./model_ckpt/')\n",
    "print(\"bert model and tokenizer initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer, length, classes):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = length\n",
    "        self.classes = classes\n",
    "        self.class2id = {class_: id for id, class_ in enumerate(self.classes)}\n",
    "        self.id2class = {id: class_ for class_, id in self.class2id.items()}\n",
    "        \n",
    "    def multi_labels_to_ids(self, labels: list[str]) -> list[float]:\n",
    "        ids = [0.0] * len(self.class2id)  # BCELoss requires float as target type\n",
    "        for label in labels:\n",
    "            ids[self.class2id[label]] = 1.0\n",
    "        return ids\n",
    "    \n",
    "    def tokenize_function(self, example):\n",
    "        result = self.tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        result[\"label\"] = torch.tensor([self.multi_labels_to_ids(eval(label)) for label in example[\"label\"]])\n",
    "        return result\n",
    "        \n",
    "data_files = {\n",
    "        \"test\": test_short_path,\n",
    "    }\n",
    "\n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer_bert, MAX_POSITION_EMBEDDINGS, classes)\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset = dataset.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=1)\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "print(\"dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.bert_model = model_bert\n",
    "        self.can_generate = model_bert.can_generate\n",
    "        self.base_model_prefix = model_bert.base_model_prefix\n",
    "        self.get_input_embeddings = model_bert.get_input_embeddings\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, 50)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "    \n",
    "model_bert = BERTClass()\n",
    "model_bert.load_state_dict(torch.load(CKPT))\n",
    "model_bert = model_bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_ICD10_Pipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            max_length = MAX_POSITION_EMBEDDINGS,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        ids = model_inputs['input_ids'].to(self.device, dtype = torch.long)\n",
    "        mask = model_inputs['attention_mask'].to(self.device, dtype = torch.long)\n",
    "        token_type_ids = model_inputs['token_type_ids'].to(self.device, dtype = torch.long)\n",
    "        outputs = self.model(ids, mask, token_type_ids).to(self.device)\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        probs = F.sigmoid(model_outputs).detach().cpu().numpy() # if there's more than one possible diagnosis\n",
    "\n",
    "        output = []\n",
    "        for i, prob in enumerate(probs[0]):\n",
    "            label = self.model.config.id2label[i]\n",
    "            score = prob\n",
    "            output.append({\"label\": label, \"score\": score})\n",
    "        # print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for faithfulness calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = BERT_ICD10_Pipeline(model=model_bert, tokenizer=tokenizer_bert, device = device)\n",
    "print(\"pipeline initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.sample(shap_input, 2)\n",
    "# shap_values = explainer(\n",
    "#         shap_input,\n",
    "#         batch_size=5,\n",
    "#         outputs=shap.Explanation.argsort.flip[:2]\n",
    "#         )\n",
    "# print(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_arrays_shap(inputs, pred_func, model, tokenizer, top_k = 5):\n",
    "    \"\"\" Function to create the arrays corresponding to the shap \n",
    "    \n",
    "    The output is in the format [[input_index_0, input_index_0, ... input_index_n, input_index_n], \n",
    "    [rationale_token_index_0 (for input 0), rationale_token_index_1 (for input 0), ... rationale_token_index_k-1 (for input n), rationale_token_index_k (for input n)]]. \n",
    "    This is used as an indexing array for masking.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the shap values over the inputs\n",
    "    shap_values = explainer(inputs, batch_size=5)\n",
    "    \n",
    "    # get the mode inferences for the inputs\n",
    "    inferences = pred_func(inputs, model, tokenizer)\n",
    "    indices_array = None\n",
    "    # get the longest \n",
    "    \n",
    "    for i, val in enumerate(shap_values):\n",
    "        # get the choosen labels\n",
    "        print(\"Inferences: \", inferences)\n",
    "        choosen_labels = np.where(inferences[i] > 0.5)\n",
    "        choosen_labels = np.unique(choosen_labels)\n",
    "        \n",
    "        # convert the indices to labels\n",
    "        choosen_labels = np.array(choosen_labels).astype(int)\n",
    "        print(\"Choosen label: \", choosen_labels)\n",
    "        choosen_labels = [id2class[label] for label in choosen_labels]\n",
    "        print(id2class)\n",
    "        print(choosen_labels)\n",
    "        \n",
    "        # for each shap value, index in via it's choosen labels\n",
    "        total_top_k_indices = np.array([])\n",
    "        top_val = min(top_k, len(choosen_labels))\n",
    "        print(top_val)\n",
    "        print(choosen_labels)\n",
    "        \n",
    "        \n",
    "        for label in choosen_labels:\n",
    "            # get the top k shap value indices\n",
    "            print(label)\n",
    "            top_k_indices = np.argsort(shap_values[i, :, label].values)[-top_val:]\n",
    "            print(top_k_indices)\n",
    "            total_top_k_indices = np.append(total_top_k_indices, top_k_indices)\n",
    "        \n",
    "        # sort the indices array to be in ascending order\n",
    "        total_top_k_indices = np.sort(total_top_k_indices)\n",
    "        # remove duplicates\n",
    "        total_top_k_indices = np.unique(total_top_k_indices)\n",
    "        # this might be wrong, it seems like shap returns indices outside of the token range\n",
    "        # so I'm not sure if shap is using the same tokenization function as ours.\n",
    "        total_top_k_indices = total_top_k_indices[total_top_k_indices < 2048]\n",
    "        \n",
    "        # create a array of the same shape of total_top_k_indices and fill with value i\n",
    "        index_array = np.full(total_top_k_indices.shape, i)\n",
    "        \n",
    "        if i == 0:\n",
    "            indices_array = [index_array.tolist(), total_top_k_indices.tolist()]\n",
    "        else:\n",
    "            # append index array to indices array[0]\n",
    "            indices_array[0] = indices_array[0] + index_array.tolist()\n",
    "            # append total_top_k_indices to indices array[1]\n",
    "            indices_array[1] = indices_array[1] + total_top_k_indices.tolist()\n",
    "    \n",
    "    return np.array(indices_array).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_model_token(texts, model, tokenizer_bert):\n",
    "    # print(len(texts))\n",
    "    # tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    # print(type(tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS)))\n",
    "    # print(\"token_att: \", dir(tk))\n",
    "    tk = tokenizer_bert(\n",
    "            texts,\n",
    "            max_length = MAX_POSITION_EMBEDDINGS,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "    ids = tk['input_ids'].to(device, dtype = torch.long)\n",
    "    mask = tk['attention_mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = tk['token_type_ids'].to(device, dtype = torch.long)\n",
    "    outputs = model_bert(ids, mask, token_type_ids)\n",
    "    # tensor_logits = outputs[0]\n",
    "    # probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    probas = F.sigmoid(outputs).detach().cpu().numpy()\n",
    "    return probas\n",
    "\n",
    "def predictor_model_no_token(texts, model, tokenizer_bert):\n",
    "    # print(len(texts))\n",
    "    # tk = tokenizer(texts, return_tensors=\"pt\",truncation=True, padding=True, max_length=MAX_POSITION_EMBEDDINGS).to(device)\n",
    "    # tokenization is removed but still need to set texts to device\n",
    "    # i'm not sure why this is a list and don't have time to debug\n",
    "    # print(\"Texts_type:\", type(texts))\n",
    "    # print(\"Texts_dir:\",  dir(texts))\n",
    "    # texts.to(device)\n",
    "    # outputs = model(**texts)\n",
    "    # tensor_logits = outputs[0]\n",
    "    # probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    ids = texts['input_ids'].to(device, dtype = torch.long)\n",
    "    mask = texts['attention_mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = texts['token_type_ids'].to(device, dtype = torch.long)\n",
    "    outputs = model_bert(ids, mask, token_type_ids)\n",
    "    tensor_logits = outputs\n",
    "    probas = tensor_logits.sigmoid().detach().cpu().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11849\n",
      "shap computed\n",
      "input type:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_5963/77823113.py\", line 22, in <module>\n",
      "    indices_array = get_index_arrays_shap(test_data, predictor_model_token, model_bert, tokenizer_bert)\n",
      "  File \"/tmp/ipykernel_5963/666571880.py\", line 11, in get_index_arrays_shap\n",
      "    shap_values = explainer(inputs, batch_size=5)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 128, in __call__\n",
      "    return super().__call__(\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/explainers/_explainer.py\", line 266, in __call__\n",
      "    row_result = self.explain_row(\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 174, in explain_row\n",
      "    self.owen(fm, self._curr_base_value, f11, max_evals - 2, outputs, fixed_context, batch_size, silent)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/explainers/_partition.py\", line 284, in owen\n",
      "    fout = fm(batch_masks)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py\", line 70, in __call__\n",
      "    return self._full_masking_call(masks, batch_size=batch_size)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py\", line 147, in _full_masking_call\n",
      "    outputs = self.model(*joined_masked_inputs)\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/shap/models/_transformers_pipeline.py\", line 30, in __call__\n",
      "    pipeline_dicts = self.inner_model(list(strings))\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/h/u15/c9/00/lokejuan/Documents/CSC413-Project/venv/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PartitionExplainer explainer: 2it [00:46, 46.85s/it]               \n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[289]\n",
      "indices_array: [[  0]\n",
      " [289]]\n",
      "instance other removed typed:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "instance other removed two typed:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "rational removed type:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "other removed type:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "batch type:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "batch type:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "Currently interpreting instance:  0\n",
      "Calculating Sufficiency\n",
      "{'input_ids': tensor([[  101,  2673,   131,   175,  1555,   131, 24928, 11955,  6385, 26206,\n",
      "          1183,  1155,  1200, 19310,   131,  1185,  1227,  1155,  1200, 19310,\n",
      "           120, 16798,  3850,  9535,   168,   168,   168,   119,  2705, 12522,\n",
      "           131, 16320,  1116,  1558, 13467,  1137, 19849,  7791,   131,   168,\n",
      "           168,   168,   118,  4841, 13335,  6617, 18965,  1348,   172, 23851,\n",
      "         12355,  4527,  1111,  1231, 25461,  1104,   172,  9014, 23033,  1197,\n",
      "          8241,  1988,  1607,  1104,  1675,  6946,   131,   168,   168,   168,\n",
      "          1110,   170,   168,   168,   168,  2130,  1114,   177,  1775, 21831,\n",
      "          1126,  8816,  6834,  1306, 13500,  2624,  1107,   168,   168,   168,\n",
      "          1150,  8218,  1121,   184,  2737,  1114,  1286,   172,  9014, 23033,\n",
      "          1197,   177,  1183,  5674,  2883, 13730,  6995,  1111, 10311,  8241,\n",
      "          1988,   119,  5351,  3756,  1115,  1210,  2277,  2403,  1131,  1408,\n",
      "          1515, 16320,  1116,   117,  1134,  1110, 22832,  1111,  1123,   119,\n",
      "          1131,  4856,  1103, 16320,  1116,  1106,  1129,  4265,  1105, 10820,\n",
      "          1114,   189, 12415,  2728,  1233,   117,  1133,  1120,  1103,  4997,\n",
      "          1108,  1126,   168,   168,   168,   119,  1131,  1145,  3756,  1515,\n",
      "          7262,  3179,   117,  1134,  1145,  1408,  1164,  1210,  2277,  2403,\n",
      "           119,  1131,  4856,  1123,  3179,  1112,   107,   188, 21365, 27910,\n",
      "          1334,  1106,  1334,   119,   107,  1131, 26360,  1251,  4152,  2607,\n",
      "           117, 22882,   117, 26979,  1158,   117,  6406,   117,  1137,  1937,\n",
      "          4006,  7262,   119,  1131,  1486,  1123,  2552,  3995,  1142,  2106,\n",
      "          1111,  9285,  3143,   117,  1150,  2752,  1123,  1106,  1103,  5048,\n",
      "          1111, 10540,  1104,  1292,  8006,   119,   184,  2737,   172,  1204,\n",
      "          2799,  1126,  1298,  1104,   177,  1183,  5674,  2883, 13730,  1107,\n",
      "          1103,  1286,   172,  9014, 14545,  1818,   117,  6995,  1111, 10311,\n",
      "          8241,  1988,   119,  1131,  1108,  2886,  3175,  1106,   168,   168,\n",
      "           168,   119,  1104,  3805,   117,  5351,  3756,  1123,  1126,  8816,\n",
      "          6834,  1306, 13500,  1110,  1136,   182,  2047, 12173,   119,  1763,\n",
      "          2657,  1607,   131,   118,   168,   168,   168,   118,   177,     1,\n",
      "         23826,  1988,   118,   188,   120,   185,  1126,  8816,  6834,  1306,\n",
      "         13500,  2624,   168,   168,   168,  1120,   168,   168,   168,  1118,\n",
      "           173,  1197,   119,   168,   168,   168,  1934,  1607,   131,   168,\n",
      "           168,   168,  1266,  1607,   131,  1185,  1227,  1607,  1104,  6625,\n",
      "           117,  4182,   117,  1126,  8816,  6834,  1306,   119,  2952, 12211,\n",
      "           131,  1113, 10296,   131,   184,   131,   189,   131,  5311,   119,\n",
      "           130,   171,  1643,   131,  7029,   120,  5073,   177,  1197,   131,\n",
      "          3324,   187,  1479,   184,  1477, 28027,  1116,  5103,   110,   187,\n",
      "          1161,   176,  1424,   131,   192,  1181,   120,   192,  1179,   117,\n",
      "          6062,   117,  9468,  1181,   119,  1119,  3452,   131,  7035,   131,\n",
      "           181,   168,   168,   168,   117,   187,   168,   168,   168,   174,\n",
      "         17112,  1554,  2455,   131, 28117,  8661,  1513,   119,  4252,  7877,\n",
      "          1306,   131,  3258,  1105,  1218,   118,  1679, 21089,   119, 24928,\n",
      "         11955,   131,  4910,  2781,   131,  8264,  1105, 10427,   117, 14561,\n",
      "          1114, 12211,   117,  2999,  6975,   119, 10592,   131,  7779,  1106,\n",
      "          1825,   117,  1282,   117,  1105,  2236,   119,  1846,   131,  4055,\n",
      "         21785,  1114,  1363,  3254,  1643, 17121,  1105, 27753,   119, 10505,\n",
      "          9964,   119,  1185,   173,  6834,  9349,  8167,  1465,  1137, 18311,\n",
      "         20695,  5053,  1665, 11122,   119,   172, 23851,  1348, 10846,   131,\n",
      "           178,   131,  1136,  7289, 25550,   131,  1286, 11602,   126,   118,\n",
      "           125,  6262,   117,  1268,   125,   118,   124,  6262,   117,  1241,\n",
      "          7808, 26844,  1106,  1609,   119, 25550,  1182,   117,   178,  1964,\n",
      "           117,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1.]])}\n",
      "instances batch size:  torch.Size([1, 512])\n",
      "Calculating Comprehensiveness\n",
      "{'input_ids': tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1, 24312,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1.]])}\n",
      "instances batch size:  torch.Size([1, 512])\n",
      "\n",
      "-- Metrics -------------------------------------------------------------\n",
      "\n",
      "\n",
      "Faithfulness for iteration:  0.07150787\n",
      "Comprehensiveness for iteration:  1.1473147\n",
      "Sufficency for iteration:  0.062326293\n",
      "\n",
      "\n",
      "Comprehensiveness Median:  1.1473147\n",
      "Comprehensiveness q1 (25% percentile):  1.1473146677017212\n",
      "Comprehensiveness q3 (75% percentile):  1.1473146677017212\n",
      "\n",
      "\n",
      "Sufficency Median:  0.062326293\n",
      "Sufficency q1 (25% percentile):  0.06232629343867302\n",
      "Sufficency q3 (75% percentile):  0.06232629343867302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faithfulness\n",
    "# this reimports the library for easy testing in the notebook\n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(faithfulness)\n",
    "\n",
    "MAX_LEN=512\n",
    "\n",
    "\n",
    "# tokenize the test dataset\n",
    "test_data =  dataset['test']['text'][0:1]\n",
    "print(len(test_data))\n",
    "print(len(test_data[0]))\n",
    "\n",
    "masker = shap.maskers.Text(pipeline.tokenizer)\n",
    "explainer = shap.Explainer(pipeline, masker)\n",
    "print(\"shap computed\")\n",
    "\n",
    "inputs = tokenizer_bert(test_data, max_length=MAX_LEN, padding='max_length', truncation=True, return_tensors='pt')\n",
    "print(\"input type: \", type(inputs))\n",
    "\n",
    "indices_array = get_index_arrays_shap(test_data, predictor_model_token, model_bert, tokenizer_bert)\n",
    "print(\"indices_array:\", indices_array)\n",
    "\n",
    "inputs_rationale_removed = faithfulness.remove_rationale_words(inputs, indices_array, join=False, tokenized=True)\n",
    "inputs_other_removed = faithfulness.remove_other_words(inputs, indices_array, join=False, tokenized=True)\n",
    "\n",
    "# print(\"rational removed: \", inputs_rationale_removed)\n",
    "# print(\"other removed: \", inputs_other_removed)\n",
    "print(\"rational removed type: \", type(inputs_rationale_removed))\n",
    "print(\"other removed type: \", type(inputs_other_removed))\n",
    "\n",
    "ind, faith = faithfulness.calculate_faithfulness(inputs, [inputs_rationale_removed], [inputs_other_removed ], model_bert, tokenizer_bert, predictor_model_no_token, tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.07150787]\n"
     ]
    }
   ],
   "source": [
    "print(ind)\n",
    "print(faith)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
