{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import accelerate\n",
    "import pytorch_lightning as pl\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel, DataCollatorWithPadding, EvalPrediction, TrainingArguments, Trainer, OPTForSequenceClassification, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "import tqdm.notebook as tq\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from __future__ import print_function\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 2048\n",
    "MODEL = \"facebook/opt-350m\"\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-05\n",
    "# LEARNING_RATE = 5e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", cache_dir='./model_ckpt/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer, MAX_LEN):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = MAX_LEN\n",
    "        self.classes = [class_ for class_ in labels_10_top50[\"icd_code\"] if class_]\n",
    "        self.class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "        self.id2class = {id: class_ for class_, id in class2id.items()}\n",
    "        \n",
    "    def multi_labels_to_ids(self, labels: list[str]) -> list[float]:\n",
    "        ids = [0.0] * len(self.class2id)  # BCELoss requires float as target type\n",
    "        for label in labels:\n",
    "            ids[self.class2id[label]] = 1.0\n",
    "        return ids\n",
    "    \n",
    "    def tokenize_function(self, example):\n",
    "        result = self.tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        result[\"label\"] = torch.tensor([self.multi_labels_to_ids(eval(label)) for label in example[\"label\"]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"output/\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train_10_top50.csv'\n",
    "test_path = 'data/test_10_top50.csv'\n",
    "val_path = 'data/val_10_top50.csv'\n",
    "labels_path = 'data/icd10_codes_top50.csv'\n",
    "\n",
    "train_10_top50 = pd.read_csv(train_path)\n",
    "val_10_top50 = pd.read_csv(val_path)\n",
    "test_10_top50 = pd.read_csv(test_path)\n",
    "\n",
    "train_10_top50_shorten = pd.read_csv(train_path)[:5000]\n",
    "val_10_top50_shorten = pd.read_csv(val_path)[:5000]\n",
    "test_10_top50_shorten = pd.read_csv(test_path)[:5000]\n",
    "\n",
    "train_short_path = \"data/train_10_top50_short.csv\"\n",
    "val_short_path = \"data/val_10_top50_short.csv\"\n",
    "test_short_path = \"data/test_10_top50_short.csv\"\n",
    "\n",
    "train_10_top50_shorten.to_csv(train_short_path, index=False)\n",
    "val_10_top50_shorten.to_csv(val_short_path, index=False)\n",
    "test_10_top50_shorten.to_csv(test_short_path, index=False)\n",
    "\n",
    "labels_10_top50 = pd.read_csv('data/icd10_codes_top50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [class_ for class_ in labels_10_top50[\"icd_code\"] if class_]\n",
    "class2id = {class_: id for id, class_ in enumerate(classes)}\n",
    "id2class = {id: class_ for class_, id in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "        \"train\": train_short_path,\n",
    "        \"validation\": val_short_path,\n",
    "        \"test\": test_short_path,\n",
    "    }\n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer, MAX_LEN)\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset = dataset.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=1)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(classes),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    return_unused_kwargs=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = OPTForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(\"model_ckpt/icd-9-shard20-checkpoint-1182/\")\n",
    "model = PeftModel.from_pretrained(model,\n",
    "                                  \"model_ckpt/icd-9-shard20-checkpoint-1182/\",\n",
    "                                  is_trainable=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, data=dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, dataset['train'], feature_names=id2class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
